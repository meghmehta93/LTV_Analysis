{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree, datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_table = pd.read_csv(\"data/driver_ids.csv\")\n",
    "ride_table = pd.read_csv(\"data/ride_ids.csv\")\n",
    "ride_ts_table = pd.read_csv(\"data/ride_timestamps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum onboarding date: \", driver_table[['driver_onboard_date']].sort_values('driver_onboard_date').head(1)['driver_onboard_date']) \n",
    "print(\"Maximum onboarding date: \", driver_table[['driver_onboard_date']].sort_values('driver_onboard_date', ascending=False).head(1)['driver_onboard_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum ride date: \", ride_ts_table[['timestamp']].sort_values('timestamp').head(1)['timestamp']) \n",
    "print(\"Maximum ride date: \", ride_ts_table[['timestamp']].sort_values('timestamp', ascending=False).head(1)['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of drivers: \", len(np.unique(driver_table['driver_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rides: \", len(np.unique(ride_table['ride_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fare = 2\n",
    "cost_per_mile = 1.15\n",
    "cost_per_minute = 0.22\n",
    "service_fee = 1.75\n",
    "minimum_fare = 5\n",
    "maximum_fare = 400\n",
    "meters_to_miles = 1609.34\n",
    "seconds_to_minutes = 60\n",
    "#Lyft take 20% of a driver's earnings.\n",
    "lyft_share = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Earnings and normalize data units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_table['ride_distance_miles'] = ride_table['ride_distance'] / meters_to_miles\n",
    "ride_table['ride_duration_minute'] = ride_table['ride_duration'] / seconds_to_minutes\n",
    "ride_table['ride_earnings'] = np.minimum(np.maximum(minimum_fare, (ride_table['ride_distance_miles'] * cost_per_mile + \n",
    "                                  ride_table['ride_duration_minute'] * cost_per_minute\n",
    "                                  + base_fare + service_fee) * (ride_table['ride_prime_time']/100.0 + 1)), maximum_fare) * lyft_share\n",
    "ride_table['unconstrained_ride_earnings'] = ride_table['ride_distance_miles'] * cost_per_mile + ride_table['ride_duration_minute'] * cost_per_minute + base_fare + service_fee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Prime Time statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (ride_table.groupby(['driver_id', 'ride_prime_time']).count() / ride_table.groupby(['driver_id']).count())[['ride_distance_miles']].reset_index().rename(columns = {'ride_distance_miles': 'percent_contribution_prime'})\n",
    "prime = pd.pivot_table(x, index = 'driver_id', columns = 'ride_prime_time', values = 'percent_contribution_prime')\n",
    "prime.fillna(0, inplace = True)\n",
    "prime = prime.reset_index()\n",
    "prime.columns = prime.columns.astype(str)\n",
    "prime['prime_work'] = pd.DataFrame(prime[prime.columns.difference(['0'])].sum(axis = 1), columns = {'prime_percentage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days since onboarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_date = ride_ts_table.groupby('ride_id')['timestamp'].max()\n",
    "ride_date = pd.DataFrame(ride_date).reset_index()\n",
    "ride_table = pd.merge(ride_table, ride_date, on = 'ride_id')\n",
    "ride_table = pd.merge(ride_table, driver_table, on = 'driver_id')\n",
    "ride_table['days_since_onboarded'] = (pd.to_datetime(ride_table['timestamp']) - pd.to_datetime(ride_table['driver_onboard_date'])).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_table['month_since_onboarded'] = np.floor((ride_table['days_since_onboarded'] )/ 31)\n",
    "ride_table['biweek_since_onboarded'] = np.floor((ride_table['days_since_onboarded'] )/ 14)\n",
    "ride_table['week_since_onboarded'] = np.floor((ride_table['days_since_onboarded'] )/ 7)\n",
    "ride_table['max_time'] = (pd.to_datetime(ride_table['timestamp']).max() - pd.to_datetime(ride_table['driver_onboard_date'])).astype('timedelta64[D]') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the time between driver events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_ts_table[\"time\"] = pd.to_datetime(ride_ts_table[\"timestamp\"],dayfirst=True)\n",
    "group_ride_table = ride_ts_table.groupby('ride_id')['time']\n",
    "ride_ts_table['time_diff'] = group_ride_table.diff()\n",
    "ride_ts_table['time_seconds'] = ride_ts_table['time_diff'].fillna(0) / pd.datetools.timedelta(seconds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days between rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_timings = pd.pivot_table(ride_ts_table, index = 'ride_id', columns = 'event', values = 'time_seconds')\n",
    "result = ride_table.sort_values(['driver_id','days_since_onboarded'])\\\n",
    "           .groupby(['driver_id'])['days_since_onboarded']\\\n",
    "           .diff().fillna(0)\n",
    "df = ride_table.sort_values(['driver_id','days_since_onboarded'])\n",
    "df['days_diff_between_rides'] = result\n",
    "print('95th percentile days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().quantile(0.95))\n",
    "print('75th percentile days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().quantile(0.75))\n",
    "print('50th percentile days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().quantile(0.5))\n",
    "print('25th percentile days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().quantile(0.25))\n",
    "print('Max days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().max())\n",
    "print('Min days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().min())\n",
    "print('Mean days diff between rides: ', df.groupby('driver_id')['days_diff_between_rides'].max().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rides_1st_2weeks = ride_table[ride_table['biweek_since_onboarded'] == 0].groupby('driver_id')['ride_id'].count().reset_index()\n",
    "count_prime_rides_1st_2weeks = ride_table[np.logical_and(ride_table['biweek_since_onboarded'] == 0, ride_table['ride_prime_time'] > 0)].groupby('driver_id')['ride_id'].count().reset_index()\n",
    "total_earnings_1st_2weeks = ride_table[ride_table['biweek_since_onboarded'] == 0].groupby('driver_id')['ride_earnings'].sum().reset_index()\n",
    "drivers = pd.DataFrame(np.unique(ride_table[ride_table['days_since_onboarded'] >= 14]['driver_id']))\n",
    "drivers.rename(columns ={0:'driver_id'}, inplace=True)\n",
    "count_rides_1st_2weeks.rename(columns={'ride_id': 'Number Rides first two weeks'}, inplace=True)\n",
    "count_prime_rides_1st_2weeks.rename(columns={'ride_id': 'Number Prime Rides first two weeks'}, inplace=True)\n",
    "total_earnings_1st_2weeks.rename(columns={'ride_earnings': 'Earnings first two weeks'}, inplace=True)\n",
    "drivers_1 = pd.merge(drivers, count_rides_1st_2weeks, on='driver_id', how = 'inner')\n",
    "drivers_2 = pd.merge(drivers_1, count_prime_rides_1st_2weeks, on='driver_id', how = 'inner')\n",
    "drivers_3 = pd.merge(drivers_2, total_earnings_1st_2weeks, on='driver_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df[np.logical_and(df['max_time'] >= 60, df['month_since_onboarded'] == 0 )]\n",
    "final = pd.merge(final, ride_timings.reset_index(), on = 'ride_id')\n",
    "final = pd.merge(final, prime.reset_index(), on='driver_id')\n",
    "# final = pd.merge(final, drivers_3, on='driver_id', how = 'inner')\n",
    "final.rename(columns={'accepted_at': 'time_between_request_and_accept', 'arrived_at': 'time_between_accept_arrive', \n",
    "            'picked_up_at': 'time_between_arrive_pickup', 'dropped_off_at': 'time_between_pickup_dropoff'}, inplace=True)\n",
    "ride_table_3 = final[np.logical_and(final['max_time'] >= 60, final['month_since_onboarded'] == 0 )]\n",
    "\n",
    "temp = ride_table_3.groupby(['driver_id']).mean().reset_index()[['driver_id', 'time_between_request_and_accept', 'time_between_accept_arrive', \n",
    "                                                                  'time_between_arrive_pickup', 'time_between_pickup_dropoff', \n",
    "                                                                    'ride_distance_miles', 'ride_duration_minute']]\n",
    "x = pd.merge(temp, drivers_3, on = 'driver_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_definition = pd.merge(pd.DataFrame(ride_table.groupby('driver_id')['days_since_onboarded'].max()).reset_index(), \n",
    "         pd.DataFrame(ride_table.groupby('driver_id')['max_time'].max()).reset_index(),\n",
    "         on = 'driver_id')\n",
    "churn_definition['churned'] = (churn_definition['max_time'] - churn_definition['days_since_onboarded']) > 31\n",
    "dataset = pd.merge(x, churn_definition[['driver_id', 'churned']], on = 'driver_id', how = 'inner')\n",
    "print(dataset['churned'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "final_data = dataset\n",
    "scores =[]\n",
    "rf = RandomForestClassifier() \n",
    "X = final_data[dataset.columns[2:len(dataset.columns)-1]]\n",
    "y = final_data['churned']\n",
    "msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[msk]\n",
    "X_test = X[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "rf.fit(X_train, y_train)\n",
    "y_score = rf.predict_proba(X_test)[:,1]\n",
    "scores.append(roc_auc_score(y_test, y_score, average=None))\n",
    "print(\"ROC/AUC score: \", scores[0])\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = final_data[dataset.columns[2:len(dataset.columns)-1]].columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = dataset\n",
    "np.random.seed(2)\n",
    "\n",
    "scores =[]\n",
    "X = final_data[dataset.columns[2:len(dataset.columns)-1]]\n",
    "y = final_data['churned']\n",
    "msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[msk]\n",
    "X_test = X[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "\n",
    "dtree=DecisionTreeClassifier(class_weight={0: 1, 1: 3}, criterion='gini', max_depth=5,\n",
    "            max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
    "            min_samples_split=25, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=25, splitter='best')\n",
    "dtree.fit(X_train, y_train) \n",
    "y_predict = dtree.predict_proba(X_test)[:,1]\n",
    "print(\"ROC/AUC score: \",roc_auc_score(y_test, y_predict, average=None))\n",
    "with open(\"churn.dot\", \"w\") as f:\n",
    "    f = tree.export_graphviz(dtree, out_file=f, feature_names = X_train.columns)\n",
    "#Can see the results in webgraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                         feature_names=X_train.columns,  \n",
    "                         class_names=['not churn', 'churned'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTV Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wasn't working well - results too poor (not enough data to validate)\n",
    "\n",
    "dataset_2 = pd.merge(x, ride_table.groupby(['driver_id'])['ride_earnings'].sum().reset_index(), on = 'driver_id')\n",
    "np.random.seed(15)\n",
    "\n",
    "\n",
    "final_data = dataset_2\n",
    "scores =[]\n",
    "dtree = tree\n",
    "clf = dtree.DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=10, min_samples_split=5, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, random_state=40, \n",
    "                                  max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                  presort=False)\n",
    "X = final_data[dataset_2.columns[1:len(dataset.columns)-2]]\n",
    "y = final_data['ride_earnings']\n",
    "msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[msk]\n",
    "X_test = X[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "clf.fit(X_train, y_train)\n",
    "y_score = clf.predict(X_test)\n",
    "print(r2_score(y_score, y_test.reset_index()['ride_earnings']))\n",
    "print(np.sqrt(mean_squared_error(y_score, y_test.reset_index()['ride_earnings'])))\n",
    "with open(\"LTV.dot\", \"w\") as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, feature_names = X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTV Prediction RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wasn't working well - results too poor (not enough data to validate)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=40)\n",
    "rf.fit(X_train, y_train)\n",
    "print \"Features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), X_train.columns), \n",
    "             reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = rf.predict(X_test)\n",
    "print(r2_score(y_score, y_test.reset_index()['ride_earnings']))\n",
    "print(np.sqrt(mean_squared_error(y_score, y_test.reset_index()['ride_earnings'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.hist(ride_table['ride_earnings'], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of Lyft Driver earnings per ride\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean monthly earnings for people for who we have most data.\n",
    "ride_table_2 = ride_table[ride_table['max_time'] >= 90]\n",
    "cohort_analysis = ride_table_2.groupby(['driver_id', 'month_since_onboarded'])['ride_earnings'].sum().reset_index()\n",
    "cohort_analysis.groupby('month_since_onboarded')['ride_earnings'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard LTV calculation: \",  230 / 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(driver_table['driver_onboard_date'].max(), driver_table['driver_onboard_date'].min())\n",
    "pd.to_datetime(driver_table['driver_onboard_date'].max()) - pd.to_datetime(driver_table['driver_onboard_date'].min())\n",
    "print(pd.to_datetime(driver_table['driver_onboard_date'].min()) + datetime.timedelta(days=12))\n",
    "print(pd.to_datetime(driver_table['driver_onboard_date'].min()) + datetime.timedelta(days=24))\n",
    "print(pd.to_datetime(driver_table['driver_onboard_date'].min()) + datetime.timedelta(days=36))\n",
    "print(pd.to_datetime(driver_table['driver_onboard_date'].min()) + datetime.timedelta(days=48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-weekly cohorts\n",
    "cohort = [None,None,None,None]\n",
    "cohort[0] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-03-28', ride_table['driver_onboard_date'] < '2016-04-09')]\n",
    "cohort[1] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-09', ride_table['driver_onboard_date'] < '2016-04-21')]\n",
    "cohort[2] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-21', ride_table['driver_onboard_date'] < '2016-05-03')]\n",
    "cohort[3] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-05-03', ride_table['driver_onboard_date'] <= '2016-05-15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly cohorts\n",
    "\n",
    "# cohort = [None,None]\n",
    "\n",
    "# cohort[0] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-03-28', ride_table['driver_onboard_date'] < '2016-04-21')]\n",
    "# cohort[1] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-21', ride_table['driver_onboard_date'] < '2016-05-15')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = []\n",
    "new = []\n",
    "transactions = []\n",
    "money = []\n",
    "for i in range(0, len(cohort)):\n",
    "    active.append([])\n",
    "    new.append([])\n",
    "    transactions.append([])\n",
    "    money.append([])\n",
    "    for j in range(0, 6):\n",
    "        new[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['biweek_since_onboarded'] == 0])))\n",
    "        if j >= 4 and cohort[i]['max_time'].quantile(0.01) < 80:\n",
    "            continue\n",
    "        elif j >= 2 and cohort[i]['max_time'].quantile(0.01) < 50:\n",
    "            continue\n",
    "        active[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['biweek_since_onboarded'] == j])))\n",
    "        transactions[i].append(len(np.unique(cohort[i]['ride_id'][cohort[i]['biweek_since_onboarded'] == j])))\n",
    "        money[i].append(np.sum(cohort[i]['ride_earnings'][cohort[i]['biweek_since_onboarded'] <= j]))\n",
    "cohort_active_users = pd.DataFrame(active)\n",
    "cohort_active_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'biweek 0',\n",
    "                                     1: 'biweek 1',\n",
    "                                     2: 'biweek 2',\n",
    "                                    3: 'biweek 3',\n",
    "                                    4: 'biweek 4',\n",
    "                                    5: 'biweek 5',\n",
    "                                    6: 'biweek 6'}\n",
    "                           , inplace=True)\n",
    "cohort_new_users = pd.DataFrame(new)\n",
    "cohort_new_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'biweek 0',\n",
    "                                     1: 'biweek 1',\n",
    "                                     2: 'biweek 2',\n",
    "                                    3: 'biweek 3',\n",
    "                                    4: 'biweek 4',\n",
    "                                    5: 'biweek 5',\n",
    "                                    6: 'biweek 6'}\n",
    "                           , inplace=True)\n",
    "cohort_transactions = pd.DataFrame(transactions)\n",
    "cohort_transactions.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'biweek 0',\n",
    "                                     1: 'biweek 1',\n",
    "                                     2: 'biweek 2',\n",
    "                                    3: 'biweek 3',\n",
    "                                    4: 'biweek 4',\n",
    "                                    5: 'biweek 5',\n",
    "                                    6: 'biweek 6'}\n",
    "                           , inplace=True)\n",
    "cohort_money = pd.DataFrame(money)\n",
    "cohort_money.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'biweek 0',\n",
    "                                     1: 'biweek 1',\n",
    "                                     2: 'biweek 2',\n",
    "                                    3: 'biweek 3',\n",
    "                                    4: 'biweek 4',\n",
    "                                    5: 'biweek 5',\n",
    "                                    6: 'biweek 6'}\n",
    "                           , inplace=True)\n",
    "CustomerRetention = cohort_active_users / cohort_new_users\n",
    "TransactionsPerCustomer = cohort_transactions/cohort_active_users\n",
    "AmountPerTransaction = cohort_money/cohort_transactions\n",
    "HistoricCLV=cohort_money/cohort_new_users\n",
    "print(CustomerRetention)\n",
    "print(TransactionsPerCustomer)\n",
    "print(AmountPerTransaction)\n",
    "print(HistoricCLV)\n",
    "print(cohort_active_users)\n",
    "LTV = pd.DataFrame((HistoricCLV * cohort_active_users).sum()) / pd.DataFrame(cohort_active_users.sum())\n",
    "\n",
    "LTV.rename(columns ={0: 'HistoricLTV'}, inplace='True')\n",
    "LTV['Percent Increase'] = LTV.diff() / LTV.shift(1)\n",
    "print(LTV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LTV['HistoricLTV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Percent increase in LTV every 2 weeks')\n",
    "plt.xticks([0,1,2,3,4,5], ['2 weeks', '4 weeks', '6 weeks', '8 weeks', '10 weeks', '12 weeks'], rotation='vertical')\n",
    "plt.plot([0,1,2,3,4,5], LTV['Percent Increase'] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model to predict LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "x = [[1],[2],[3],[4], [5]]\n",
    "# Train the model using the training sets\n",
    "regr.fit(x, np.log(LTV['Percent Increase'])[1:].reset_index()['Percent Increase'].reshape(-1,1))\n",
    "\n",
    "# Make predictions using the testing set\n",
    "# diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_, regr.intercept_)\n",
    "x_test = []\n",
    "for i in range(1, 100):\n",
    "    x_test.append([i])\n",
    "multiple = reduce(lambda x, y: x*y, np.exp(regr.predict(x_test)) + 1)\n",
    "multiple_2 = reduce(lambda x, y: x*y, np.exp(regr.predict(x_test))[5:] + 1)\n",
    "print(\"LTV: \", multiple_2 * 633.483576)\n",
    "#multiple * 141.583565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly cohort tracking (Should be more accurate measure of churn but more limited data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = []\n",
    "new = []\n",
    "transactions = []\n",
    "money = []\n",
    "for i in range(0, len(cohort)):\n",
    "    active.append([])\n",
    "    new.append([])\n",
    "    transactions.append([])\n",
    "    money.append([])\n",
    "    for j in range(0, 3):\n",
    "        new[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['month_since_onboarded'] == 0])))\n",
    "        if j == 2 and cohort[i]['max_time'].quantile(0.1) < 80:\n",
    "            continue\n",
    "        elif j == 1 and cohort[i]['max_time'].quantile(0.1) < 50:\n",
    "            continue\n",
    "        active[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['month_since_onboarded'] == j])))\n",
    "        transactions[i].append(len(np.unique(cohort[i]['ride_id'][cohort[i]['month_since_onboarded'] == j])))\n",
    "        money[i].append(np.sum(cohort[i]['ride_earnings'][cohort[i]['month_since_onboarded'] <= j]))\n",
    "cohort_active_users = pd.DataFrame(active)\n",
    "cohort_active_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'Month 0',\n",
    "                                     1: 'Month 1',\n",
    "                                     2: 'Month 2'}\n",
    "                           , inplace=True)\n",
    "cohort_new_users = pd.DataFrame(new)\n",
    "cohort_new_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'Month 0',\n",
    "                                     1: 'Month 1',\n",
    "                                     2: 'Month 2'}\n",
    "                           , inplace=True)\n",
    "cohort_transactions = pd.DataFrame(transactions)\n",
    "cohort_transactions.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'Month 0',\n",
    "                                     1: 'Month 1',\n",
    "                                     2: 'Month 2'}\n",
    "                           , inplace=True)\n",
    "cohort_money = pd.DataFrame(money)\n",
    "cohort_money.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                   1: 'Cohort 04-09 - 04-21', \n",
    "                                   2: 'Cohort 04-21 - 05-03', \n",
    "                                   3: 'Cohort 05-03 - 05-15'},\n",
    "                           columns= {0: 'Month 0',\n",
    "                                     1: 'Month 1',\n",
    "                                     2: 'Month 2'}\n",
    "                           , inplace=True)\n",
    "CustomerRetention = cohort_active_users / cohort_new_users\n",
    "TransactionsPerCustomer = cohort_transactions/cohort_active_users\n",
    "AmountPerTransaction = cohort_money/cohort_transactions\n",
    "HistoricCLV=cohort_money/cohort_new_users\n",
    "print(CustomerRetention)\n",
    "print(TransactionsPerCustomer)\n",
    "print(AmountPerTransaction)\n",
    "print(HistoricCLV)\n",
    "LTV = pd.DataFrame((HistoricCLV * cohort_active_users).sum()) / pd.DataFrame(cohort_active_users.sum())\n",
    "\n",
    "LTV.rename(columns ={0: 'HistoricLTV'}, inplace='True')\n",
    "LTV['Percent Increase'] = LTV.diff() / LTV.shift(1)\n",
    "LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "x = [[1], [2]]\n",
    "# Train the model using the training sets\n",
    "regr.fit(x, np.log(LTV['Percent Increase'])[1:3].reset_index()['Percent Increase'].reshape(-1,1))\n",
    "# regr.fit(x, LTV['Percent Increase'][1:3].reshape(-1,1))\n",
    "\n",
    "# Make predictions using the testing set\n",
    "# diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "plt.plot([1,2], np.log(LTV['Percent Increase'])[1:3])\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_, regr.intercept_)\n",
    "x_test = []\n",
    "for i in range(1, 100):\n",
    "    x_test.append([i])\n",
    "multiple = reduce(lambda x, y: x*y, np.exp(regr.predict(x_test)) + 1)\n",
    "multiple_2 = reduce(lambda x, y: x*y, np.exp(regr.predict(x_test))[2:] + 1)\n",
    "print(\"LTV from monthly view: \" , multiple_2 * 652) \n",
    "#multiple * 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "248 * 0.85 * 0.85,248 * 0.85 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Life Time of driver: \", 1 / 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not working because probably not enough data.\n",
    "\n",
    "# from lifelines import KaplanMeierFitter\n",
    "# kmf = KaplanMeierFitter() \n",
    "# cohort = [None,None,None,None]\n",
    "# cohort[0] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-03-28', ride_table['driver_onboard_date'] < '2016-04-09')]\n",
    "# cohort[1] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-09', ride_table['driver_onboard_date'] < '2016-04-21')]\n",
    "# cohort[2] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-21', ride_table['driver_onboard_date'] < '2016-05-03')]\n",
    "# cohort[3] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-05-03', ride_table['driver_onboard_date'] <= '2016-05-15')]\n",
    "# churn_definition = pd.merge(pd.DataFrame(cohort[0].groupby('driver_id')['days_since_onboarded', 'month_since_onboarded'].max()).reset_index(), \n",
    "#          pd.DataFrame(cohort[0].groupby('driver_id')['max_time'].max()).reset_index(),\n",
    "#          on = 'driver_id')\n",
    "# churn_definition['churned'] = (churn_definition['max_time'] - churn_definition['days_since_onboarded']) > 23\n",
    "# # The 1st arg accepts an array or pd.Series of individual survival times\n",
    "# # The 2nd arg accepts an array or pd.Series that indicates if the event \n",
    "# # interest (or death) occured.\n",
    "# final = churn_definition\n",
    "\n",
    "# kmf.fit(durations = final['month_since_onboarded'], \n",
    "#         event_observed =final['churned'])\n",
    "# kmf.event_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, exponweib\n",
    "rv = exponweib(4, 2)\n",
    "rv.pdf(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, exponweib\n",
    "rv = exponweib(4, 0.9)\n",
    "rv.pdf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, exponweib\n",
    "rv = exponweib(4, 0.9)\n",
    "x = range(3, 20)\n",
    "\n",
    "m = reduce(lambda x, y: x*y, rv.pdf(x) + 1)\n",
    "m * 652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, exponweib\n",
    "rv = exponweib(4, 2)\n",
    "x = range(3, 20)\n",
    "\n",
    "m = reduce(lambda x, y: x*y, rv.pdf(x) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "cohort[0] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-03-28', ride_table['driver_onboard_date'] < '2016-04-09')]\n",
    "cohort[1] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-09', ride_table['driver_onboard_date'] < '2016-04-21')]\n",
    "cohort[2] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-04-21', ride_table['driver_onboard_date'] < '2016-05-03')]\n",
    "cohort[3] = ride_table[np.logical_and(ride_table['driver_onboard_date'] >= '2016-05-03', ride_table['driver_onboard_date'] <= '2016-05-15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rides_1st_2weeks = ride_table[ride_table['biweek_since_onboarded'] == 0].groupby('driver_id')['ride_id'].count().reset_index()\n",
    "count_prime_rides_1st_2weeks = ride_table[np.logical_and(ride_table['biweek_since_onboarded'] == 0, ride_table['ride_prime_time'] > 0)].groupby('driver_id')['ride_id'].count().reset_index()\n",
    "total_earnings_1st_2weeks = ride_table[ride_table['biweek_since_onboarded'] == 0].groupby('driver_id')['ride_earnings'].sum().reset_index()\n",
    "drivers = pd.DataFrame(np.unique(ride_table[ride_table['days_since_onboarded'] >= 14]['driver_id']))\n",
    "drivers.rename(columns ={0:'driver_id'}, inplace=True)\n",
    "count_rides_1st_2weeks.rename(columns={'ride_id': 'Number Rides first two weeks'}, inplace=True)\n",
    "count_prime_rides_1st_2weeks.rename(columns={'ride_id': 'Number Prime Rides first two weeks'}, inplace=True)\n",
    "total_earnings_1st_2weeks.rename(columns={'ride_earnings': 'Earnings first two weeks'}, inplace=True)\n",
    "drivers_1 = pd.merge(drivers, count_rides_1st_2weeks, on='driver_id', how = 'inner')\n",
    "drivers_2 = pd.merge(drivers_1, count_prime_rides_1st_2weeks, on='driver_id', how = 'inner')\n",
    "drivers_3 = pd.merge(drivers_2, total_earnings_1st_2weeks, on='driver_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rides_1st_2weeks['1st2weeksDistance'] = count_rides_1st_2weeks['Number Rides first two weeks'] > 42.5\n",
    "count_prime_rides_1st_2weeks['not_enough_prime'] = count_prime_rides_1st_2weeks['Number Prime Rides first two weeks'] <= 5\n",
    "ride_timings['too_long_topickup'] = ride_timings['arrived_at'] <= 369.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_analysis(num_rides_1st_2weeks, cohort):\n",
    "    cohort[0] = num_rides_1st_2weeks[np.logical_and(num_rides_1st_2weeks['driver_onboard_date'] >= '2016-03-28', num_rides_1st_2weeks['driver_onboard_date'] < '2016-04-09')]\n",
    "    cohort[1] = num_rides_1st_2weeks[np.logical_and(num_rides_1st_2weeks['driver_onboard_date'] >= '2016-04-09', num_rides_1st_2weeks['driver_onboard_date'] < '2016-04-21')]\n",
    "    cohort[2] = num_rides_1st_2weeks[np.logical_and(num_rides_1st_2weeks['driver_onboard_date'] >= '2016-04-21', num_rides_1st_2weeks['driver_onboard_date'] < '2016-05-03')]\n",
    "    cohort[3] = num_rides_1st_2weeks[np.logical_and(num_rides_1st_2weeks['driver_onboard_date'] >= '2016-05-03', num_rides_1st_2weeks['driver_onboard_date'] <= '2016-05-15')]\n",
    "    active = []\n",
    "    new = []\n",
    "    transactions = []\n",
    "    money = []\n",
    "    for i in range(0, len(cohort)):\n",
    "        active.append([])\n",
    "        new.append([])\n",
    "        transactions.append([])\n",
    "        money.append([])\n",
    "        for j in range(0, 3):\n",
    "            new[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['month_since_onboarded'] == 0])))\n",
    "            if j == 2 and cohort[i]['max_time'].quantile(0.1) < 80:\n",
    "                continue\n",
    "            elif j == 1 and cohort[i]['max_time'].quantile(0.1) < 50:\n",
    "                continue\n",
    "            active[i].append(len(np.unique(cohort[i]['driver_id'][cohort[i]['month_since_onboarded'] == j])))\n",
    "            transactions[i].append(len(np.unique(cohort[i]['ride_id'][cohort[i]['month_since_onboarded'] == j])))\n",
    "            money[i].append(np.sum(cohort[i]['ride_earnings'][cohort[i]['month_since_onboarded'] <= j]))\n",
    "    cohort_active_users = pd.DataFrame(active)\n",
    "    cohort_active_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                       1: 'Cohort 04-09 - 04-21', \n",
    "                                       2: 'Cohort 04-21 - 05-03', \n",
    "                                       3: 'Cohort 05-03 - 05-15'},\n",
    "                               columns= {0: 'Month 0',\n",
    "                                         1: 'Month 1',\n",
    "                                         2: 'Month 2'}\n",
    "                               , inplace=True)\n",
    "    cohort_new_users = pd.DataFrame(new)\n",
    "    cohort_new_users.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                       1: 'Cohort 04-09 - 04-21', \n",
    "                                       2: 'Cohort 04-21 - 05-03', \n",
    "                                       3: 'Cohort 05-03 - 05-15'},\n",
    "                               columns= {0: 'Month 0',\n",
    "                                         1: 'Month 1',\n",
    "                                         2: 'Month 2'}\n",
    "                               , inplace=True)\n",
    "    cohort_transactions = pd.DataFrame(transactions)\n",
    "    cohort_transactions.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                       1: 'Cohort 04-09 - 04-21', \n",
    "                                       2: 'Cohort 04-21 - 05-03', \n",
    "                                       3: 'Cohort 05-03 - 05-15'},\n",
    "                               columns= {0: 'Month 0',\n",
    "                                         1: 'Month 1',\n",
    "                                         2: 'Month 2'}\n",
    "                               , inplace=True)\n",
    "    cohort_money = pd.DataFrame(money)\n",
    "    cohort_money.rename(index={0:'Cohort 03-28 - 04-09', \n",
    "                                       1: 'Cohort 04-09 - 04-21', \n",
    "                                       2: 'Cohort 04-21 - 05-03', \n",
    "                                       3: 'Cohort 05-03 - 05-15'},\n",
    "                               columns= {0: 'Month 0',\n",
    "                                         1: 'Month 1',\n",
    "                                         2: 'Month 2'}\n",
    "                               , inplace=True)\n",
    "    CustomerRetention = cohort_active_users / cohort_new_users\n",
    "    TransactionsPerCustomer = cohort_transactions/cohort_active_users\n",
    "    AmountPerTransaction = cohort_money/cohort_transactions\n",
    "    HistoricCLV=cohort_money/cohort_new_users\n",
    "    print(CustomerRetention)\n",
    "    print(TransactionsPerCustomer)\n",
    "    print(AmountPerTransaction)\n",
    "    print(HistoricCLV)\n",
    "    LTV = pd.DataFrame((HistoricCLV * cohort_active_users).sum()) / pd.DataFrame(cohort_active_users.sum())\n",
    "    print(cohort_active_users)\n",
    "    LTV.rename(columns ={0: 'HistoricLTV'}, inplace='True')\n",
    "    LTV['Percent Increase'] = LTV.diff() / LTV.shift(1)\n",
    "    print(LTV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than > 42.5 rides in the first 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, count_rides_1st_2weeks[count_rides_1st_2weeks['1st2weeksDistance'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less than 42.5 rides in the first 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, count_rides_1st_2weeks[count_rides_1st_2weeks['1st2weeksDistance'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less than 5 prime rides in the first 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, count_prime_rides_1st_2weeks[count_prime_rides_1st_2weeks['not_enough_prime'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than 5 prime time rides in the first 2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, count_prime_rides_1st_2weeks[count_prime_rides_1st_2weeks['not_enough_prime'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greater than 1 day between rides on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.groupby('driver_id')['days_diff_between_rides'].mean().reset_index()\n",
    "df_1['not_very_active'] = df_1['days_diff_between_rides'] <= 1\n",
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, df_1[df_1['not_very_active'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less than one day on average between rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, df_1[df_1['not_very_active'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than 3 minutes on average between accepted ride and arrived to pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_timings_2 = pd.merge(ride_timings.reset_index(), ride_table, on = 'ride_id')\n",
    "ride_timings_2 = ride_timings_2.groupby('driver_id')['arrived_at'].median().reset_index()\n",
    "ride_timings_2['too_long_topickup'] = ride_timings_2['arrived_at'] <= 180\n",
    "ride_timings_2['too_long_topickup'].sum(), ride_timings_2['too_long_topickup'].count()\n",
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_topickup'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less than 3 minutes on average between accepted ride and arrived to pick up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_topickup'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time between request and accept greater than 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_timings_2 = pd.merge(ride_timings.reset_index(), ride_table, on = 'ride_id')\n",
    "ride_timings_2 = ride_timings_2.groupby('driver_id')['accepted_at'].median().reset_index()\n",
    "ride_timings_2['too_long_wait'] = ride_timings_2['accepted_at'] <= 5\n",
    "ride_timings_2['too_long_wait'].sum(), ride_timings_2['too_long_wait'].count()\n",
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_wait'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time between request and accept less than 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_wait'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time between arriving and picking-up passenger less than 2.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_timings_2 = pd.merge(ride_timings.reset_index(), ride_table, on = 'ride_id')\n",
    "ride_timings_2 = ride_timings_2.groupby('driver_id')['picked_up_at'].mean().reset_index()\n",
    "ride_timings_2['too_long_wait'] = ride_timings_2['picked_up_at'] >= 2.4\n",
    "ride_timings_2['too_long_wait'].sum(), ride_timings_2['too_long_wait'].count()\n",
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_wait'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time between arriving and picking-up passenger greater than 2.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_timings_2[ride_timings_2['too_long_wait'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ride Duration in Minutes greater than 13 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_table_2 = ride_table.groupby('driver_id')['ride_duration_minute'].mean().reset_index()\n",
    "ride_table_2['too_short_ride'] = ride_table_2['ride_duration_minute'] < 13\n",
    "ride_table_2['too_short_ride'].sum(), ride_table_2['too_short_ride'].count()\n",
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_table_2[ride_table_2['too_short_ride'] == 0], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ride Duration in Minutes less than 13 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, ride_table_2[ride_table_2['too_short_ride'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = [None,None,None,None]\n",
    "num_rides_1st_2weeks = pd.merge(ride_table, count_prime_rides_1st_2weeks[count_prime_rides_1st_2weeks['not_enough_prime'] == 0], on = 'driver_id', how = 'inner') \n",
    "num_rides_1st_2weeks = pd.merge(num_rides_1st_2weeks, count_rides_1st_2weeks[count_rides_1st_2weeks['1st2weeksDistance'] == 1], on = 'driver_id', how = 'inner') \n",
    "cohort_analysis(num_rides_1st_2weeks, cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
